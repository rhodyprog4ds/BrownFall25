---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.17.3
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Nonlinear Regression

```{code-cell} ipython3
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
sns.set_theme(font_scale=2,palette='colorblind')
```

We will use the same data
```{code-cell} ipython3
test_samples = 20
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)
X_train,X_test, y_train,y_test = train_test_split(diabetes_X, diabetes_y ,
                                                  test_size=test_samples,random_state=0)
```

And retrain a model like [Tuesday](#firstdblr)
```{code-cell} ipython3
regr_db = linear_model.LinearRegression()
regr_db.fit(X_train, y_train)
```
and again score it

```{code-cell} ipython3
:label:dbmoredata
y_pred = regr_db.predict(X_test)
regr_db.score(X_test, y_test)
```

This is better than [Tuesday's score](#dbfullscore). 

This time it is better just by **using more data to train**

```{code-cell} ipython3
train_samples,_ = X_train.shape
total_samples, _ = diabetes_X.shape
train_samples, train_samples
```

Above we used an integer to the `test_size` parameter so we set the *number* of samples instead of the percentage of the data.  We used {eval}`test_samples` for testing, which is only {eval}`str(np.round((test_samples/total_samples)*100,2))`% of the data.  Which is a lot less than the 25%  used before, so wwith more training data we can get a better model. 


## Polynomial Regression



Polynomial regression is still a linear problem.  Linear regression solves for
the $\beta_i$ for a $d$ dimensional problem.

$$ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_d x_d = \sum_i^d \beta_i x_i$$

Quadratic regression solves for

$$ y = \beta_0 + \sum_i^d \beta_i x_i + \sum_j^d \sum_i^d \beta_{d+i} x_i x_j + \sum_i^d x_i^2 $$

This is still a linear problem, because we can create a new $X'$ matrix that has the
polynomial values of each feature  and solve for more $\beta$ values.

So if our original features are $x_1, x_2, \ldots, x_d$ our new $X'$ will have 3 types of features original($x_1, x_2, \ldots, x_d$), squared($x_1^2, x_2^2, \ldots, x_d^2$) and interactions ($x_1x_2, x_1x_2, \ldots, x_{d-1}x_d$). 

We use a transformer object, which works similarly to the estimators, but does not use targets. 
::::{seealso}
You can learn more about data preprocessing in general from the sklearn user guide section on  [Dataset Transformations](https://scikit-learn.org/stable/data_transforms.html)
:::::

First, we instantiate.

```{code-cell} ipython3
poly = PolynomialFeatures()
```

Then we can fit transform on the training data and tranform on the test data: 
```{code-cell} ipython3
X2_train =  poly.fit_transform(X_train)
X2_test = poly.transform(X_test)
```

This changes the shape a lot, now we have a lot more features
```{code-cell} ipython3
X2_train.shape
```

::::{exercise}
:label: polyfeatcount
Can you explain why it increased to 66 from 10?
:::::

+++

:::::::{solution} polyfeatcount
:class: dropdown

We can break down this total into different types, the original ones ($x_0, x_1, \ldots, x_9$), those squared, ($x_0^2, x_1^2, \ldots, x_9^2$), every pair ($x_0x_1, x_0x_1, \ldots, x_7x_8, x_8x_9$) and a constant (so we do not need the intercept separately).  

::::{tab-set}
:::{tab} Math 

for $d$ orginal features the new total will be:

$$ 2*d + \sum_{i=1}^{d-1}i + 1$$

so for $d=10$ that is:
$$2*10 + (1+2+3+4+5+6+7+8+9) + 1$$
:::
:::{tab} Python

```{code-cell} ipython3
original_feats = 10
sq_feats = 10
pair_prod_feats =  sum(range(10)) 
const_feats = 1
original_feats + sq_feats + pair_prod_feats + const_feats
```
:::
::::

:::::::


Now we can fit a model and score ite
```{code-cell} ipython3
regr_db2 = linear_model.LinearRegression()
regr_db2.fit(X2_train, y_train)
regr_db2.score(X2_test, y_test)
```

And we get even better performance than [adding data alone](#dbmoredata) did above. 

```{code-cell} ipython3
regr_db2.coef_
```


## Loading a Pretrained model! 

In class, we went over the [installation](#share:install) and [login](#sharing:login) from the [sharing a model tutorial](../resources/sharing.md) in preparation for [A5](../assignments/05-ml.md). 

```{code-cell} ipython3
from huggingface_hub import hf_hub_download
import skops.io as sio
hf_hub_download(repo_id="CSC310-fall25/example_decision_tree", filename="model.pkl",local_dir='.')
dt_loaded = sio.load('model.pkl')
```

```{code-cell} ipython3
dt_loaded.predict(np.asarray([[5,6], [1,3]]))
```

::::{attention}
I added more tips to the [sharing a model tutorial](../resources/sharing.md) but if you still find more problems, you can still earn extra credit for commenting with your issues or (double) for sharing a solution with your problem 
::::::

## Questions After Class

:::{note}

All questions today were about the assignment so I tried to make sure that everything was more clear in the instructions

If you submit a question as an issue on the repo, you can earn extra credit. 
::::





