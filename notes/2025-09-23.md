---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.17.2
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---


# Cleaning Data - Structure


## Intro

This week, we'll be cleaning data.

Cleaning data is **labor intensive** and requires making *subjective* choices.  
We'll focus on, and assess you on, manipulating data correctly, making reasonable
choices, and documenting the choices you make carefully.

We'll focus on the programming tools that get used in cleaning data in class
this week:
- reshaping data
- handling missing or incorrect values
- renaming columns


```{code-cell} ipython3
import pandas as pd
import seaborn as sns

sns.set_theme(font_scale=2, palette='colorblind')
```



Note here we set the `theme` and pass the palette and font size as paramters.
To learn more about this and the various options, see the [seaborn aesthetics tutorial](https://seaborn.pydata.org/tutorial/aesthetics.html)


::::{attention}
If mssing a library, you can run `pip install` in a notebook cell, so for seaborn: 
```
pip install seaborn
```
:::::


## What is Tidy Data?

Read in the three csv files described below and store them in a dictionary


::::::{warning}
in class I used a list, but this way shows some extra things you might find as helpful examples, and it will make some things later a bit easier
:::::::

```{code-cell} ipython3
url_base = 'https://raw.githubusercontent.com/rhodyprog4ds/rhodyds/main/data/'

data_filename_list = ['study_a.csv','study_b.csv','study_c.csv']
```

Here we can use a dictionary comprehension

```{code-cell} ipython3
example_data = {datafile.split('.')[0].split('_')[1]:pd.read_csv(url_base+datafile) for datafile in data_filename_list}
```

- this is a *dictionary comprehension* which is like a list comprehension, but builds a
dictionary instead. in the first pass `datafile='study_a.csv'`
- `split` is a string method that splits the string at the given character and returns a list (in the first pass `['study_a','csv']`)
- `[0]` then takes the first item out `'study_a'`
- then the next `split` makes another list `['study','a']`
- `[1]` then takes the second item from the list (e.g. `'a'`)
- `:` makes the part to the left a key and right the value
- we can add strings to combine them

:::::::::{topic} Breaking down that comprehension further

```{tip}
This section also illustrates a way you can work to understand **any** piece of code:
*take small sections and run them one at a time*
```

We can do smaller examples of each step. 

First a tiny dictionary comprehension
```{code-cell} ipython3
{char:ord(char) for char in 'abcdef'}
```
this gives us the ascii number for each character in that string with the `char` as key and `ord(char)`'s *output* as the value. Note that it *runs* the code assigned in the value because of the `()` doing the function call.

Next the split method, we will use this sentence. 

```{code-cell} ipython3
sentence = 'Next the split method, we will use this sentence.'
type(sentence), len(sentence)
```

this is type `str` so we can use the [string class methods](https://docs.python.org/3/library/stdtypes.html#string-methods) in Python, such as [`split`](https://docs.python.org/3/library/stdtypes.html#str.split). 
As a `str` the `len` function tells us how many *characters* because a string is iterable over characters. 
We used this fact in the tiny dictionary above. 

By default it splits at spaces
```{code-cell} ipython3
sentence.split()
```
we can instead tell it on what string to split
```{code-cell} ipython3
clauses = sentence.split(',')
type(clauses)
```

`split` always returns a list, so we can pick the first item with `[0]`
```{code-cell} ipython3
clauses[0]
```

We can concatenate `str` objects with `+`
```{code-cell} ipython3
title = 'Dr.'
last = 'Brown'
title + last
```

:::::::::



### the same data 3 ways

We can pick a single on out this way
```{code-cell} ipython3
example_data['a']
```

```{code-cell} ipython3
example_data['b']
```

```{code-cell} ipython3
example_data['c']
```



These three all show the same data, but let's say we have two goals:
- find the average  effect per person across treatments
- find the average effect per treatment across people

This works differently for these three versions.

For `a` we can easilty get the average per treatment, but to get the 
average per person, we have to go across rows, which we can do here, but doesn't work as well with plotting

we can work across rows with the `axis` parameter if needed

For B,  we get the average per person, but what about per treatment? again we have to go across rows instead.

For the third one, however, we can use groupby, because this one is tidy. 

## Encoding Missing values

We can see the impact of a bad choice to represent a missing value with `-`
```{code-cell} ipython3
example_data['c'].dtypes
```
one non numerical value changed the whole column!

```{code-cell} ipython3
example_data['c'].describe()
```
so `describe` treats it all like categorical data

We can use the `na_values` parameter to fix this. Pandas recognizes a lot of different
values to mean missing and store as `NaN` but his is not one. Find the full list 
in the [`pd.read_csv` documentation of the `na_values` parameter](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#:~:text=of%20large%20files.-,na_values,-Hashable%2C%20Iterable%20of)

```{code-cell} ipython3
example_data = {datafile.split('.')[0].split('_')[1]:pd.read_csv(url_base+datafile,na_values='-') 
                for datafile in data_filename_list}
```
now we can checka gain
```{code-cell} ipython3
example_data['c'].dtypes
```
and we see that it is `float` this is because `NaN` is a float. 

```{code-cell} ipython3
example_data['c']
```
Now it shows as Nan here


## Computing on Tidy Data


These three all show the same data, but let's say we have two goals:

- find the average effect per person across treatments
- find the average effect per treatment across people


This works differently for these three versions.

To take the mean on both columns we can either pick them out or make the name the index. we'll do the latter:

```{code-cell} ipython3
example_data['a'].set_index('name')
```

we can take the mean down:

```{code-cell} ipython3
example_data['a'].set_index('name').mean(axis=0)
```

which is default so the above is the same as:
```{code-cell} ipython3
example_data['a'].set_index('name').mean()
```

or across
```{code-cell} ipython3
example_data['a'].set_index('name').mean(axis=1)
```

but for the tidy data (c) we can use groupby instead to get the same impact

```{code-cell} ipython3
tidy_df = example_data['c']
tidy_df
```

```{code-cell} ipython3
tidy_df.groupby('person')['result'].mean()
```

```{code-cell} ipython3
tidy_df.groupby('treatment')['result'].mean()
```

:::::{seealso}
The original [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) paper is worth reading to build a deeper understanding of these ideas.  
:::::::::

## Tidying Data


Let's reshape the first one to match the tidy one. First, we
will save it to a DataFrame, this makes things easier to read
and enables us to use the built in help in jupyter, because it can't check types too many levels into a data structure.

Before

```{code-cell} ipython3
treat_df = example_data['a']
treat_df
```

```{code-cell} ipython3
tidy_treat_df = treat_df.melt(value_vars=['treatmenta','treatmentb'],
             id_vars=['name'],value_name='result',
             var_name='treatment')
tidy_treat_df
```


When we melt a dataset:
- the `id_vars` stay as columns
- the data from the `value_vars` columns become the values in the `value` column
- the column names from the `value_vars` become the values in the `variable` column
- we can rename the value and the variable columns.


[see visualized on pandas tutor](https://pandastutor.com/vis.html#code=import%20pandas%20as%20pd%0Aimport%20io%0A%0Acsv%20%3D%20'''%0Aname,treatmenta,treatmentb%0AJohn%20Smith,,2%0AJane%20Doe,16,11%0AMary%20Johnson,3,1%0A'''%0A%0Astudy_a%20%3D%20pd.read_csv%28io.StringIO%28csv%29%29%0A%0Astudy_a.melt%28id_vars%3D'name',var_name%3D'treatment',value_name%3D'result'%29&d=2024-09-25&lang=py&v=v1)


## Transforming the Coffee data

First we see another way to filter data. We can use statistics of the data to filter and remove some. . 

For exmaple, lets only keep the data from the top 10 countries in terms of number of bags. 

```{code-cell} ipython3
arabica_data_url = 'https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv'
# load the data
coffee_df = pd.read_csv(arabica_data_url,index_col=0)
# get total bags per country
bags_per_country = coffee_df.groupby('Country.of.Origin')['Number.of.Bags'].sum()

# sort descending, keep only the top 10 and pick out only the country names
top_bags_country_list = bags_per_country.sort_values(ascending=False)[:10].index

# filter the original data for only the countries in the top list
top_coffee_df = coffee_df[coffee_df['Country.of.Origin'].isin(top_bags_country_list)]
```

::::{tip}
Inspect the variables of the cell above to try to better understand how it works
:::::


First we will look at the head, as usual
```{code-cell} ipython3
top_coffee_df.head()
```

and the shape to see how big it is after we filter

```{code-cell} ipython3
top_coffee_df.shape
```

:::::::::{margin}
:::::{attention}
I renamed `attrs_of_interest` to `attributes_of_interest`. `attrs` was short for attributes, but it does make the code less readable, so I improved it.  Thanks for the question! 
:::::
:::::::::

We want to have some columns subsetted out for plotting

```{code-cell} ipython3
scores_of_interest = ['Balance','Aroma', 'Body', 'Aftertaste']
n_value_vars = len(scores_of_interest)
attributes_of_interest = ['Country.of.Origin', 'Color']
coffee_tall = top_coffee_df.melt(id_vars=attributes_of_interest, 
                   value_vars=scores_of_interest ,
                   var_name ='score')#

tall_rows, tall_cols = coffee_tall.shape
tall_rows, tall_cols
```
first we'll look at the shape to help make sense of what melt does. 

```{code-cell} ipython3
rows, cols = top_coffee_df.shape
```

the origal data had {eval}`rows` rows and we converted {eval}`n_value_vars` to one colum so now we have the product in number of rows

```{code-cell} ipython3
tall_rows_computed = rows* n_value_vars
tall_rows_computed 
```

and we can see that they match. 

```{code-cell} ipython3
coffee_tall.head()
```

This data is "tidy" all of the different measurements (ratings) are different rows and we have a few 
columsn that identify each sample 

This version plays much better with plots where we want to compare the ratings. 



This version plays much better with plots where we want to compare the ratings. 
````{margin}

```{tip}
Copy this cell and run it, then cut out one parameter at a time to get before/after to see more visually what each one does
```
````
```{code-cell} ipython3
sns.catplot(data = coffee_tall, row='score', y='value',x='Country.of.Origin',
            hue='Country.of.Origin',aspect=3, 
             kind='violin') 
```

This now we can use the `score` as a variable to break the data by in our subplots. 
I did a few new things in this: 
- a [violin plot](https://seaborn.pydata.org/tutorial/categorical.html#violinplots) allows us to compare how different data is distributed. 
- I used both `col` and `col_wrap`, `col` alone would have made nine columns, and without `row` all in one row. `col_wrap` says to line wrap after a certain number of columns (here 3)
- `aspect` is an attribute of the [figure level plots](https://seaborn.pydata.org/tutorial/function_overview.html#figure-level-vs-axes-level-functions) that controls the ratio of width to height (the aspect ratio). so `aspect=3` makes it 3 times as wide as tall in each subplot, this spreads out the x tick labels so can read them 


:::::::{exercise}
Next, we'll deal with how to fix the values within  the data.  To see the
types of things:

- [Stanford Policy Lab Open POlicing Project data readme](https://github.com/stanford-policylab/opp/blob/master/data_readme.md)
- [Propublica Machine Bias](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) the "How we acquired data" section

Read through them both to get a general sense about the things that they had to fix. Try to find at least 5-6 examples of different things they fixed in each case. Make sure you find some examples where what to do is not obviously correct and you think something else could be justified in some case. 
:::::::


## Questions After Class 

Today's questions were mostly about melt.  Read above and the linked resources. 

### Can you change the locations of the names next to a plot? 

First, see the annotated graph and learn the technical name of the element you want to move. 


![annotated graph](https://matplotlib.org/stable/_images/sphx_glr_anatomy_001.png)

the above figure come from [matplotlib's Anatomy of a Figure](https://matplotlib.org/stable/gallery/showcase/anatomy.html) page which includes the *code* to generate that figure

You can control each of these but you may need to use `matplotlib`. 

If this means the legend, seaborn can control that location. If it refers to fixing overlappign tick labels, I demonstrated that above, with `aspect`. 