---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.17.3
kernelspec:
  display_name: rhodyds
  language: python
  name: rhodyds
---

# Regression

::::::{attention}
[Assignment 5](../assignments/05-ml.md) has a different structure. 

There is a chance for [extra credit](https://github.com/rhodyprog4ds/BrownFall25/issues/3) by testing the [sharing instructions](../resources/sharing.md)
:::::::::::::

## Setting up for regression


We're going to predict **tip** from **total bill**.
This is a regression problem because the target, *tip* is:
 1. available in the data (makes it supervised) and 
 2.  a continuous value.
The problems we've seen so far were all classification, species of iris and the
character in that corners data were both categorical.  

Using linear regression is also a good choice because it makes sense that the tip
would be approximately linearly related to the total bill, most people pick some
percentage of the total bill.  If we our prior knowledge was that people
typically tipped with some more complicated function, this would not be a good
model.


```{code-cell} ipython3
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
sns.set_theme(palette='colorblind')
```

```{code-cell} ipython3
:tags: [hide-cell]
def pretty_type(object):
    ''' 
    return minimal types for complex object
    '''
    full_type = str(type(object)).replace('class','').strip("'<>' ")
    full_type_list = full_type.split('.')
    return ' '.join([full_type_list[0], full_type_list[-1]])
```

We wil load this data from `seaborn`

```{code-cell} ipython3
tips_df = sns.load_dataset('tips')
tips_df.head()
```

We will, for now, use only the `total_bill` to predict the `tip`

```{code-cell} ipython3
tips_X = tips_df[['total_bill']]
tips_y = tips_df['tip']
pretty_type(tips_df[['total_bill']])
```

::::::::{admonition} Why two brackets? 
The double `[[]]` means that we make a {eval}`type(['total_bill'])` first and use that to select from the `tips_df`[^typeft]. This means we get a whole {eval}`pretty_type(tips_df[['total_bill']])` instead of a {eval}`pretty_type(tips_df['total_bill'])`

[^typeft]: here, the type is computed from code, hover ove the type words to see what object was typed. 


This is important because when we use sklearn with a pandas object (Series or DataFrame), it picks out the values, like this: 
```{code-cell} ipython3
tips_df['total_bill'].values
```

This is because originally sklearn actually only worked on `numpy` arrays and so what they do now is pull it out of the pandas object, it gives us a numpy array. 

All sklearn estimator objects are designed to work on multiple {term}`features <feature>`, which mean they **requires**  that the features have a 2D shape, even if there is only 1 feature and this not 2D.  

However, picking out only one column, by default has 1 dimension: 
```{code-cell} ipython3
tips_df['total_bill'].values.shape, tips_df['total_bill'].values.ndim
```

there is only 1 number in the shape, it only has a number of rows. 

but this way is: 
```{code-cell} ipython3
tips_df[['total_bill']].values.shape, tips_df[['total_bill']].values.ndim
```

this way has a 1 in the second position.  It is a bit counter-intuitive that a single column has two different representations like this, but it is how numpy works. You can learn more in the [numpy guide](https://numpy.org/doc/stable/user/absolute_beginners.html#how-do-you-know-the-shape-and-size-of-an-array)


We could also fix this by adding an axis.  It doesn't change values, but changes the way it is represented enough that it has the properties we need. 

```{code-cell} ipython3
tips_df['total_bill'].values[:,np.newaxis].ndim
```

::::::::::::::::


Next, we split the data


```{code-cell} ipython3
tips_X_train,tips_X_test, tips_y_train, tips_y_test = train_test_split(tips_X, tips_y,
                                                                       train_size=.8)
```


## Fitting a regression model

We instantiate the object as we do with all other sklearn estimator objects. 

```{code-cell} ipython3
regr = linear_model.LinearRegression()
```


we can inspect it to get a before
```{code-cell} ipython3
regr.__dict__
```

and `fit` like the others too: 
```{code-cell} ipython3
regr.fit(tips_X_train,tips_y_train)
```
and compare the after

```{code-cell} ipython3
regr.__dict__
```

We can also save the predictions 
```{code-cell} ipython3
tips_y_pred = regr.predict(tips_X_test)
```

and get a $R^2$ score for the model

```{code-cell} ipython3
regr.score(tips_X_test,tips_y_test)
```

or the mean squared error: 


```{code-cell} ipython3
mean_squared_error(tips_y_pred,tips_y_test)
```




## Regression Predictions

:::::{attention}
I skipped this in class, but if you are unfamiliar with the concept of regression you should review this carefully
:::::::::::

Linear regression is making the assumption that the target is a linear function of the features so that we can use the equation for a line(for scalar variables): 

$$ y_i =mx_i + b$$ 

becomes equivalent to the following code assuming they were all vectors/matrices:
```
i = 1 # could be any number from 0 ... len(target)
target[i] = regr.coef_*features[i]+ regr.intercept_
```

You can match these up one for one. 
-  `target` is $y$ (this is why we usually put the `_y_` in the varaible name)
-  `regr.coef_` is the slope, $m$
-  `features` are the $x$ (like for y, we label that way)
-  and `regr.intercept_` is the $b$ or the y intercept. 

We will look at each of them and store them to variables here


```{code-cell} ipython3
slope = regr.coef_[0]
intercept = regr.intercept_
```


:::::::{note}
This is what our model *predicts* the tip will be based on the past data.  It is
important to note that this is not what the tip *should* be by any sort of
virtues. For example, a typical normative rule for tipping is to tip 15% or 20%.
the model we learned, from this data, however is {eval}`str(slope.round(2)*100)`% + {eval}`str(intercept.round(2))`
:::::::::

::::::{margin}
::::{tip}
Note that my random seed is not fixed, so the values there are going to change each time the
website is updated, but that these values will *always* be true because I used the `myst` `{eval}` role to pull the values from the variable into the text. 
:::::
::::::::

now we can pull out our first $x_i$ and calculate a predicted value of $y$ manually

```{code-cell} ipython3
x0 = tips_X_test.iloc[0].values[0]
manual_pred0 = slope * x0 + intercept
model_pred0 = tips_y_pred[0]
manual_pred0 == model_pred0
```

and see that the manual prediction ({eval}`manual_pred0`) matches exactly the prediction from the `predict` method ({eval}`model_pred0`)[^hovereval]. 

[^hovereval]: the numbers in this sentence are inserted with code so you can hover to see what variables they came from. 

## Visualizing Regression

Since we only have one feature, we can visualize what was learned here.  We will use plain
`matplotlib` plots because we are plotting from numpy arrays not data frames. 


```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
```

This plots the predictions in blue vs the real data in black.  The above uses them both as scatter plots to make it more clear, below, like in class, I'll use a normally inconvenient aspect of a line plot to show all the predictions the model could make in this range.  

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.plot(tips_X_test,tips_y_pred, color='blue')
```


## Evaluating Regression - Mean Squared Error

From the plot, we can see that there is some error for each point, so accuracy
that we've been using, won't work.  One idea is to look at how much error there
is in each prediction, we can look at that visually first, these are called the {term}`residuals`. 

::::::{admonition} What happened to the plot in class? 
:class: dropdown

We tried to plot the residuals in class:

```{code-cell} ipython3
plt.scatter(tips_X_test, tips_y_test,  color='black')
plt.plot(tips_X_test, tips_y_pred, color='blue', linewidth=3)

# draw vertical lines frome each data point to its predict value
[plt.plot([x,x],[yp,yt], color='red', linewidth=3)
                 for x, yp, yt in zip(tips_X_test, tips_y_pred,tips_y_test)];
```

but it didn't look right. 

I should have noticed from the `total_bill` on the horizontal axis what was going on it ws plotting the `y` values vs `total_bill`.  We can confirm that is what happened by looking at what the data provided to the plot function is:

```{code-cell} ipython3

[[[x,x],[yp,yt]] for x, yp, yt in zip(tips_X_test, tips_y_pred,tips_y_test)]
```

then we can fix it by getting out the values instead as I do below.
::::::::::

```{code-cell} ipython3
plt.plot(tips_X_test, tips_y_pred, color='blue', linewidth=3, label ='predictions')

# draw vertical lines frome each data point to its predict value
[plt.plot([x,x],[yp,yt], color='red', linewidth=3)
                 for x, yp, yt in zip(tips_X_test.values, tips_y_pred,tips_y_test)];

plt.plot([x0, x0],[tips_y_pred[0],tips_y_test.iloc[0]], color='red', linewidth=3, label='residual')

# plot these last so they are visually on top
plt.scatter(tips_X_test, tips_y_test,  color='black', label='data')
plt.legend(loc=2)
plt.xlabel('total bill ($)')
plt.ylabel('tip ($)')
```

In this code block:
- I use `zip` a 
[builtin function in Python](https://docs.python.org/3/library/functions.html#zip) 
to iterate over all of the test samples and predictions (they're all the same length) 
and plot each red line in a list comprehension.  This could have been a for loop, but
the comprehension is slighly more compact visually.  The zip allows us to have a Pythonic,
easy to read loop that iterates over multiple variables.  
- To make a vertical line, I make a line plot with just two values.  My plotted data is:
`[x,x],[yp,yt]`  so the first point is `(x,yp)` and the second is `x,yt`.  
- The `;` at the end of each line suppressed the text output. (try removing it to see what it does)



We can use the average length of these red lines to capture the error. To get
the length, we can take the difference between the prediction and the data for
each point. Some would be positive and others negative, so we will square each
one then take the average.  This will be the  {term}`mean squared error`

```{code-cell} ipython3
mean_squared_error(tips_y_test,tips_y_pred)
```

Which is equivalent to: 

```{code-cell} ipython3
np.mean((tips_y_test-tips_y_pred)**2)
```

To interpret this, we can take the square root to get it back into dollars. This becomes equivalent to taking the mean absolute value of the error. 

```{code-cell} ipython3
mean_abs_error = np.sqrt(mean_squared_error(tips_y_test,tips_y_pred))
mean_abs_error
```

Still, to know if this is a big or small error, we have to compare it to the values
we were predicting

```{code-cell} ipython3
avg_tip = tips_y_test.mean()
avg_tip
```

the average error(\${eval}`str(mean_abs_error.round(3))`) is not that big, in absolute terms, but it's large relative to the average tip (\${eval}`str(avg_tip.round(3))`) being wrong by {eval}`(mean_abs_error/avg_tip).round(4)*100`% is not great. 


(r2score_intuition)=
## Evaluating Regression - R2

We can also use the $R^2$ score, the [coefficient of determination](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score).

If we have the following:
- $n$ `=len(y_test)``
- $y$ `=y_test`
- $y_i$ `=y_test[i]`
- $\hat{y}$ = `y_pred`
- $\bar{y} = \frac{1}{n}\sum_{i=0}^n y_i$ = `sum(y_test)/len(y_test)`

$$ R^2(y, \hat{y}) = 1 - \frac{\sum_{i=0}^n (y_i - \hat{y}_i)^2}{\sum_{i=0}^n (y_i - \bar{y}_i)^2} $$ 


```{code-cell} ipython3
r2_score(tips_y_test,tips_y_pred)
```



This is a bit harder to interpret, but we can use some additional plots to
visualize.
This code simulates data by randomly picking 20 points, spreading them out
and makes the “predicted” y values by picking a slope of 3. Then I simulated various levels of noise, by sampling noise and multiplying the same noise vector by different scales and adding all of those to a data frame with the column name the r score for if that column of target values was the truth.

Then I added some columns of y values that were with different slopes and different functions of x. These all have the small amount of noise.



```{code-cell} ipython3
# decide a number of points
N = 20
# pick some random points
x = 10*np.random.random(N)
# make a line with a fixed slope
y_pred_demo = 3*x
# put it in a data frame
ex_df = pd.DataFrame(data = x,columns = ['x'])
ex_df['y_pred'] = y_pred_demo
# set some amounts of noise
n_levels = range(1,18,2)
# sample random values between 0 and 1, then make the spread between -1 and +1
noise = (np.random.random(N)-.5)*2
# add the noise and put it in the data frame with col title the r2
for n in n_levels:
    y_true = y_pred_demo + n* noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred_demo,y_true),3))] = y_true

# set some other functions
f_x_list = [2*x,3.5*x,.5*x**2, .03*x**3, 10*np.sin(x)+x*3,3*np.log(x**2)]
# add them to the noise and store with r2 col title
for fx in f_x_list:
    y_true = fx + noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred_demo,y_true),3))] = y_true    

# make the data tidy for plotting
xy_df = ex_df.melt(id_vars=['x','y_pred'],var_name='rscore',value_name='y')
xy_df.head()
```

This creates a single {term}`tidy <Tidy Data Format>` or tall dataframe with data from:
-  {eval}`N` random points in [0,1] and then scales them to be between 0-10 
- 'predicted' model that is $3x$, always.
- 'true' data that has multiple levels of noise added the loop over `n_levels`
- 'true' data from other functions `f_x_list`

Now we can plot it all: 
```{code-cell} ipython3
# make a custom grid by using facet grid directly
g = sns.FacetGrid(data = xy_df,col='rscore',col_wrap=3,aspect=1.5,height=3)
# plot the lin
g.map(plt.plot, 'x','y_pred',color='k')
# plot the dta
g.map(sns.scatterplot, "x", "y",)
```

:::::{margin}
:::{seealso}
[Facet Grids](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html) allow more customization than the figure level plotting functions (e.g. Pairplot and relplot)
we have used otherwise, but each of those combines a FacetGrid with a
particular type of plot.
:::
:::::

In these, you can see the varying levels of how much the data agrees with te prediction and the corresponding $R^2$. 


## Regression auto scoring
as all sklearn objects, it has a built in score method

```{code-cell} ipython3
regr.score(tips_X_test, tips_y_test)
```

this matches the $R^2$ score: 
```{code-cell} ipython3
r2_score(tips_y_test,tips_y_pred)
```

and is different from mse
```{code-cell} ipython3
mean_squared_error(tips_y_test,tips_y_pred)
```

```{code-cell} ipython3
tips_df.head()
```


## Mutlivariate Regression

Recall the equation for a line: 

$$ \hat{y} = mx+b$$

When we have multiple variables instead of a scalar $x$ we can have a vector $\mathbf{x}$ and instead of a single slope, we have a vector of coefficients $\beta$

$$ \hat{y} = \beta^T\mathbf{x} + \beta_0 $$

where $\beta$ is the `regr_db.coef_` and $\beta_0$ is `regr_db.intercept_` and that's a vector multiplication and $\hat{y}$ is `y_pred` and $y$ is `y_test`.  

In scalar form, a vector multiplication can be written like

$$ \hat{y} = \sum_{k=0}^d(x_k*\beta_k) + \beta_0$$

where there are $d$ features, that is $d$= `len(X_test[k])` and $k$ indexed into it. 



We can also load data from Scikit learn.

This dataset includes 10 features measured on a given date and an measure of
diabetes disease progression measured one year later. The predictor we can train
with this data might be someting a doctor uses to calculate a patient's risk.  

```{code-cell} ipython3
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y = True)
```


This model predicts what lab measure a patient will have one year in the future
based on lab measures in a given day.  Since we see that this is not a very high
r2, we can say that this is not a perfect predictor, but a Doctor, who better
understands the score would have to help interpret the core.



```{code-cell} ipython3
diabetes_X[:5]
```

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y)
```

and fit: 


```{code-cell} ipython3
:label:firstdblr
regr_diabetes = linear_model.LinearRegression()
regr_diabetes.fit(X_train, y_train)
```

We can look at the estimator again and see what it learned. It describes the model like a line:

$$ \hat{y} = mx+b$$

except in this case it's multivariate, so we can write it like:

$$ \hat{y} = \beta^Tx + \beta_0 $$

where $\beta$ is the `regr_db.coef_` and $\beta_0$ is `regr_db.intercept_` and that's a vector multiplication and $\hat{y}$ is `y_pred` and $y$ is `y_test`.  

In scalar form it can be written like

$$ \hat{y} = \sum_{k=0}^d(x_k*\beta_k) + \beta_0$$

where there are $d$ features, that is $d$= `len(X_test[k])` and $k$ indexed into it. For example in the below $k=0$


```{code-cell} ipython3
:label: dbfullscore
regr_diabetes.score(X_test, y_test)
```

```{code-cell} ipython3
regr_diabetes.__dict__
```

```{code-cell} ipython3
regr.__dict__
```


## LASSO

To solve a regular linear regression problem and find the weights $w$ to make the line:
$$ y=Xw$$

The `fit` method find the $w$ to minimize the following {term}`objective function`, for $N$ samples, : 

$$ \frac{1}{2N}||y- Xw||_2^2 $$

Note that this is basically minimizing the mean squared error. 

Lasso allows us to pick a subset of the features at the same time we learn the weights

The objective is to minimize: 
$$ \frac{1}{2N}||y- Xw||_2^2 +\alpha||w||_1 $$

or in ~python like syntax: 
```
(1 / (2 * n_samples)) * ||y - Xx||^2_2 + alpha * ||w||_1
```

It is implemented as an estimator object like every other one we have seen
```{code-cell} ipython3
lasso = linear_model.Lasso()
lasso.fit(X_train, y_train)
lasso.score(X_test, y_test)
```

We see it learns a model with most of the coefficients set to 0. 
```{code-cell} ipython3
coefs = lasso.coef_
n_coefs = len(coefs) - sum(coefs==0)
coefs
```

we can increase the number of allowed parameters by using making `alpha` smaller because  making $\alpha$ smaller allows $||w||_1$ to be bigger while keeping their product $\alpha||w||_1$ the same which is added to the total predictive error. 

```{code-cell} ipython3
lasso = linear_model.Lasso(alpha=.25)
lasso.fit(X_train, y_train)
lasso.score(X_test, y_test)
```

```{code-cell} ipython3
coefs = lasso.coef_
n_coefs = len(coefs) - sum(coefs==0)
coefs
```

```{code-cell} ipython3
lasso = linear_model.Lasso(alpha=.05)
lasso.fit(X_train, y_train)
lasso.score(X_test, y_test)
```
we can compare this to the [original score](#dbfullscore)

```{code-cell} ipython3
coefs = lasso.coef_
n_coefs = len(coefs) - sum(coefs==0)
coefs
```


## how could you inspect the residuals with more than one input feature

Let's get the predictions: 
```{code-cell} ipython3
y_pred = regr_diabetes.predict(X_test)
```

One thing we can do is plot the predictions vs the real values, this would *ideall* be a perfect line
```{code-cell} ipython3
plt.scatter(y_test, y_pred)
plt.plot(y_test,y_test,'k')
```

There is some noise as epxected, and further the errors are not uniformly distributed (the black line is the true vs itself) for lower values the model tends to over predict (the predictions are higher than the real value) and for the higher values it tends to underpredict (the predictions are mostly lower than the real value). 

We could also plot the {term}`residuals <residual>` similarly: 

```{code-cell} ipython3
plt.scatter(y_test, y_test-y_pred)
```

we see the same pattern. 


We could also plot the residuals vs the features, to do this, we will make a dataframe and use seaborn.  First, we'll get the feature names.

```{code-cell} ipython3
db_bunch = datasets.load_diabetes()
feature_names = db_bunch.feature_names
```

Now make a wide dataframe and melt it to be tall
```{code-cell} ipython3
diabetes_residuals_wide = pd.DataFrame(data = X_test, columns =feature_names)
diabetes_residuals_wide['y_pred'] = y_pred
diabetes_residuals_wide['y_test'] = y_test
diabetes_residuals_wide['residual'] = y_test - y_pred

diabetes_residuals = diabetes_residuals_wide.melt(id_vars = ['residual','y_pred','y_test'],
                        value_vars = feature_names, value_name='feature_value', var_name='feature')
diabetes_residuals.head()
```


and now we can make  subplot for each figure and plot the residual vs the features

```{code-cell} ipython3
sns.relplot(data = diabetes_residuals, x='feature_value',y = 'residual',
          col='feature',col_wrap=3)
```
From this, it actually looks like none of the individual features have a lot more information left that we could find. 


```{code-cell} ipython3
plt.scatter(tips_y_test, tips_y_test-tips_y_pred)
```


## Questions


### How does LASSO differ from regular linear regresion?

LASSO is a regularized model. 

### What is the range of $R^2$?

1 is ideal, it would be 0 if the we set the model to always predict `y=x.mean()` and it can be negative, because it can be arbitrarily bad. 