---
jupytext:
  formats: ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.17.3
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

+++

# Classification with Naive Bayes


::::{attention} 

I updated the [submission instructions](#a3submit)for A3. 

A few of you did not make a PR for A2, too that is slowing down our ability to grade them. 
:::::::

 Today,  we will see our first machine learning model: Gaussian Naive bayes. 

 This is a {term}`generative <generative model>` {term}`classification` {term}`model` 

 First let's load the modules and data that we will use today. 

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score 
import matplotlib.pyplot as plt

iris_df = sns.load_dataset('iris')

sns.set_theme(palette='colorblind')
```

## Introduction to the Iris data


We're trying to build an automatic flower classifier that, for measurements of a new flower returns the predicted species(different types of iris). To do this, we have a DataFrame with columns for species, petal width, petal length, sepal length, and sepal width. The species is what type of flower it is the petal and sepal are parts of the flower. 

::::{figure} https://www.integratedots.com/wp-content/uploads/2019/06/iris_petal-sepal-e1560211020463.png
:alt: image of an iris with the petal width, petal length, sepal length and sepal width annotated
:label: annotatediris

The petal and sepal are different parts of a flower, the popular `iris` data contains 150 examples of irises from 3 species and 4 measurements of each: petal width, petal length, sepal length and sepal width
::::::

```{code-cell} ipython3
iris_df.head()
```

The species will be the {term}`target` and the 4 measurements will be the {term}`features <feature>`.  

::::{important} 
In any classification problem the goal is to
 predict the target from the features.
::::::
 
 
For the iris data, our goal is to predict the species from the measurements. To make our code really explicit about how we are using each feature we will make variables for the features and target. 


```{code-cell} ipython3
feature_vars = ['sepal_length', 'sepal_width','petal_length', 'petal_width',]
target_var = 'species'
```

We can look at a few things first: 
```{code-cell} ipython3
classes = list(pd.unique(iris_df[target_var]))
iris_df[target_var].value_counts()
```

We see that it has {eval}`len(classes)` classes: {eval}`classes` and that they all have the same number of samples.  

We refer to this as a {term}`balanced` dataset. 

## Separating Training and Test Data

To do machine learning, we split the data both sample wise (rows if tidy) and variable-wise (columns if tidy). First, we'll designate the columns to use as features and as the target.  

The features are the input that we wish to use to predict the target.

Next, we'll use a sklearn function to split the data randomly into test and train portions.

The `train_test_split` function returns multiple values, the docs say that it returns [twice as many](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#:~:text=splittinglist%2C%20length%3D2%20*%20len(arrays)) as it is passed.

```{code-cell} ipython3
random_state_choice = 5
X_train, X_test, y_train, y_test = train_test_split(iris_df[feature_vars],
                                                    iris_df[target_var],
                                                    random_state=random_state_choice)
```
::::::{margin}
::::{tip}
the `random_seed` is optional, but to get the same result everytime you run code, it is useful. I set it here, so that the notes will always have the same performance. 
::::
::::::

We passed two separate things, the features and the labels separated, so we get train and test each for each:
- `X_train` is the {term}`feature` values for {term}`training data`
- `y_train` is the {term}`target` values for {term}`training data`
- `X_test` is the {term}`feature` values for {term}`test data`
- `y_test` is the {term}`target` values for {term}`test data`

::::{attention}
Here we used a typical math notation: 
- capital letters for matrices (more than one column)
- lowercase letters for vectors (one column)
::::

We can look at the training data's head: 

```{code-cell} ipython3
X_train.head()
```
::::{attention}
If you have a different random seed your output here will be different than mine
:::::

and the test data
```{code-cell} ipython3
X_test.head()
```

we see that they have different heads but the same columns.  Let's look at their sizes: 


```{code-cell} ipython3
full_r, _ = iris_df.shape
train_r, train_c = X_train.shape
test_r, test_c = X_test.shape
full_r, train_r, test_r
```

We see the total data has {eval}`full_r` rows and then {eval}`train_r` will be used for training and {eval}`test_r` for test

these add up: 
```{code-cell} ipython3
full_r == train_r + test_r
```

and by default the percentage of training data
```{code-cell} ipython3
train_pct = train_r/full_r
train_pct
```

We see by default it is {eval}`train_pct*100`%



## What does Gaussian Naive Bayes do?

Gaussian Naive Bayes is a classification model that assumes: 
(gnbassumptions)=
- Gaussian = the data is [distributed](#dist:gaussian)
- Naive = indepedent (uncorrelated/near 0 correlation) features (see: <wiki:Correlation>)
- Bayes = most probable class is the right one to predict

More resources:
- [Bayes Estimator](https://en.wikipedia.org/wiki/Bayes_estimator)
- [docs](https://scikit-learn.org/stable/modules/naive_bayes.html)



We can look at this data using a pair plot. It plots each pair of numerical variables in a grid of scatterplots and on the diagonal (where it would be a variable with itself) shows the distribution of that variable (with a kdeplot).

```{code-cell} ipython3
:label: irispairplot
sns.pairplot(iris_df,hue='species')
```

This data is reasonably **separable** beacause the different species (indicated with colors in the plot) do not overlap much.   All classifiers require separable classes. 

We see that the features are distributed sort of like a normal, or Gaussian, distribution. In 1D this is the [familiar bell curve](#stdnorm).  In 2D a [Gaussian distribution](#dist:multivariategaussian) is like a [hill](#mvn), so we expect to see more points near the center and fewer on the edge of circle-ish blobs.  These blobs are slightly like [ovals](#stretchmvn), but not too [skew(diagonal)](#skewmvn). 

This means that the assumptions of the Gaussian Naive Bayes model are met well enough we can expect the classifier to do well. 


### Instantiating our Model Object



:::::::{margin}
:::{seealso} Further Reading
The Scikit Learn [User Guide](https://scikit-learn.org/stable/user_guide.html) is
 a really good place to learn the details of machine learning.  It is high quality documentation from both a statistical and computer science
perspective of every element of the library.

The sklearn [API](https://scikit-learn.org/stable/modules/classes.html) describes
how the library is structured and organized. Because the library is so popular
(and it's pretty well architected from a software perspective as well) if you
are developing new machine learning techniques it's good to make them sklearn
compatible.  

For example, IBM's [AIF360](https://aif360.readthedocs.io/en/latest/index.html) is a package for doing fair machine learning which has a
[sklearn compatible interface](https://aif360.readthedocs.io/en/latest/modules/sklearn.html). Scikit Learn documentation also includes a
[related projects](https://scikit-learn.org/stable/related_projects.html) page.
::::
:::::::::
Next we will instantiate the object for our *model*.  In `sklearn` they call these objects [estimator](https://scikit-learn.org/stable/tutorial/statistical_inference/settings.html#estimators-objects). All estimators have a similar usage.  First we instantiate the object and set any *hyperparameters*.

Instantiating the object says we are assuming a particular type of model.  In this case [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html).  
(gnbassumptions)=
`GaussianNB` sets several assumptions in one form:
- we assume data are Gaussian (normally) distributed
- the features are uncorrelated/independent (Naive)
- the best way to predict is to find the highest probability (Bayes)

this is one example of a [Bayes Estimator](https://en.wikipedia.org/wiki/Bayes_estimator)



```{code-cell} ipython3
gnb = GaussianNB()
```

At this point the object is not very interesting, but we can still inspect it so we can see a before and after

```{code-cell} ipython3
gnb.__dict__
```

:::{tip}
the `__dict__` attribute serializes a python object and exists for most objects. You can read more in the Python [data model](https://docs.python.org/3/reference/datamodel.html) docs. 
:::

## Fitting the model to the data

The fit method uses the data to learn the model's parameters, it implements the {term}`learning algorithm`.  In this case, a Gaussian distribution is characterized by  a mean and variance; so the GNB classifier is characterized by one mean and one variance for each class (in 4d, like our data).

```{code-cell} ipython3
gnb.fit(X_train, y_train)
```


The attributes of the [estimator object](https://scikit-learn.org/stable/glossary.html#term-estimators) (`gbn`) describe the data (eg the class list) and the model's parameters. The `theta_` (often in math as $\theta$ or $\mu$)
represents the mean and the `var_` ($\sigma$) represents the variance of the
distributions.

```{code-cell} ipython3
gnb.__dict__
```

We can see it learned a bunch about our data.  

The `theta_` is the means of the distribution and the `var_` is the variance. 


## Scoring a model

Estimator objects also have a score method.  If the estimator is a classifier, that score is accuracy.  We will see that for other types of estimators it is different types.

```{code-cell} ipython3
:label:gnbscore
gnb.score(X_test,y_test)
```


## Making model predictions

:::::{attention}
Now that we have a fit model, we can save it and use it on new data, or built it into another application.  
We will come back to how to save models a bit later, but the idea is that we can *use* this on new data
::::::: 

we can predict for each sample as well: 

```{code-cell} ipython3
y_pred = gnb.predict(X_test)
y_pred
```

We can also do one single sample, the `iloc` attrbiute lets us pick out rows by
integer index even if that is not the actual index of the DataFrame
```{code-cell} ipython3
X_test.iloc[0]
```
but if we pick one row, it returns a series, which is incompatible with the predict method. 


```{code-cell} ipython3
:tags: ["raises-exception"]
gnb.predict(X_test.iloc[0])
```

If we select with a range, that only includes 1, it still returns a DataFrame

```{code-cell} ipython3
X_test.iloc[0:1]
```

which we can get a prediction for: 

```{code-cell} ipython3
gnb.predict(X_test.iloc[0:1])
```

We could also transform with `to_frame` and then {term}`transpose` with [`T`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.T.html#pandas.DataFrame.T) or ([`transpose`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transpose.html))
```{code-cell} ipython3
gnb.predict(X_test.iloc[0].to_frame().T)
```

We can also pass a 2D array (list of lists) with values in it (here I typed in values similar to the mean for setosa above, so it should predict setosa)
```{code-cell} ipython3
gnb.predict([[5.1, 3.6, 1.5, 0.25]])
```

This way it warns us that the feature names are missing, but it still gives a prediction. 

## More evaluation

Like we saw last week, we can also produce a confusion matrix for this problem.  This time however it will be {eval}`len(classes)`x{eval}`len(classes)` since we have 3 {term}`classes <class>`: {eval}`classes`
```{code-cell} ipython3
confusion_matrix(y_test,y_pred)
```


This is a little harder to read than the 2D version but we can make it a dataframe to read it better.

```{code-cell} ipython3
n_classes = len(gnb.classes_)
prediction_labels = [['predicted class']*n_classes, gnb.classes_]
actual_labels = [['true class']*n_classes, gnb.classes_]
conf_mat = confusion_matrix(y_test,y_pred)
conf_df = pd.DataFrame(data = conf_mat, index=actual_labels, columns=prediction_labels)
conf_df
```

We see that the setosa is never mistaken for other classes but he other two are mixed up a few times. 


A summar "report" is also available: 
```{code-cell} ipython3
print(classification_report(y_test,y_pred))
```
We can also get a report with a few metrics.

- Recall is the percent of each species that were predicted correctly.
- Precision is the percent of the ones predicted to be in a species that are truly that species.
- the F1 score is combination of the two

We see we have perfect recall and precision for setosa, as above, but we have lower for the other two because there were mistakes where versicolor and virginica were mixed up. 


## How does GNB make predictions? 

Recall the [original assumptions](#gnbassumptions).  We can see the third one (predicting most probable class) by looking at, for each test sample the probability it belongs to each class. 

For two sample we get a probability for each class: 
```{code-cell} ipython3
gnb.predict_proba(X_test.iloc[0:2])
```

We can visualize these by making a dataframe and plotting it. 

::::{tip}
These cells are hidden because they *should* be mostly just reveiw of EDA/cleaning tools
::::

First we make the dataframe with the probabilities, the true values and the predictions. 
```{code-cell} ipython3
:tags: [hide-cell]
# make the probabilities into a dataframe labeled with classes & make the index a separate column
prob_df = pd.DataFrame(data = gnb.predict_proba(X_test), columns = gnb.classes_ ).reset_index()
# add the predictions as a new column
prob_df['predicted_species'] = y_pred
# add the true species as a new column
prob_df['true_species'] = y_test.values
prob_df.head()
```

Then add some additional columns
```{code-cell} ipython3
:tags: [hide-cell]
# for plotting, make a column that combines the index & prediction
pred_text = lambda r: str( r['index']) + ',' + r['predicted_species']
prob_df['i,pred'] = prob_df.apply(pred_text,axis=1)
# same for ground truth
true_text = lambda r: str( r['index']) + ',' + r['true_species']
prob_df['i,true'] = prob_df.apply(true_text,axis=1)
# a dd a column for which are correct
prob_df['correct'] = prob_df['predicted_species'] == prob_df['true_species']
prob_df.head()
```

Finally we melt the data to use it for plotting
```{code-cell} ipython3
:tags: [hide-cell]
prob_df_melted = prob_df.melt(id_vars =[ 'index', 'predicted_species','true_species','i,pred','i,true','correct'],value_vars = gnb.classes_,
                             var_name = target_var, value_name = 'probability')
prob_df_melted.head()
```

```{code-cell} ipython3
sns.set_theme(font_scale=2)
sns.catplot(data =prob_df_melted, x = 'species', y='probability' ,col ='i,true',
            col_wrap=5,kind='bar', hue='species')
```

Here we see for each point in the test set a bar for the model's probability that it belongs to each of the {eval}`len(classes)` classes({eval}`classes`).  

Most points are nearly probability 1 in the correvt class and near 0 in the other classes. 

For example: 

```{code-cell} ipython3
prob_select_idx = prob_df_melted['index'].isin([0,2,3])
sns.catplot(data =prob_df_melted[prob_select_idx], x = 'species', 
    y='probability' ,col ='i,true',
            col_wrap=5,kind='bar', hue='species')
```

Some, however, are less clear cut, for exmple, sample 1

```{code-cell} ipython3
split = [1]
prob_select_idx = prob_df_melted['index'].isin(split)

sns.catplot(data =prob_df_melted[prob_select_idx], x = 'species', 
    y='probability' ,col ='i,true',
            col_wrap=5,kind='bar', hue='species')
```

Sample 1's true class is {eval}`y_test.iloc[1]`, but it was predicted as {eval}`y_pred[1]`.  We see in the plot, the model was actually split on this sample, 2 bars are meaningfully above 0. 

We could even pick out all of the ones where the predictionw as wrong:

```{code-cell} ipython3
error_idx = prob_df_melted['correct'] ==False

sns.catplot(data =prob_df_melted[error_idx], x = 'species', 
    y='probability' ,col ='i,true',
            col_wrap=5,kind='bar', hue='species')
```

All of these have a somewhat split decision, so that is good, the model was not confidently wrong.  


## What does it mean to be a generative model? 

Gaussian Naive Bayes is a very simple model, but it is a {term}`generative model` (in constrast to a {term}`discriminative model`) so we can use it to generate synthetic data that looks like the real data, based on what the model learned. 


```{code-cell} ipython3

sns.set_theme(font_scale=1)
N = 50
gnb_df = pd.DataFrame(np.concatenate([np.random.multivariate_normal(th, sig*np.eye(4),N)
                 for th, sig in zip(gnb.theta_,gnb.var_)]),
                 columns = gnb.feature_names_in_)
gnb_df['species'] = [ci for cl in [[c]*N for c in gnb.classes_] for ci in cl]
gnb_df.head()
```

To break this code down: 

-  we extract the mean and variance parameters from the model
(`gnb.theta_,gnb.sigma_`) 
- then `zip` them together to create an {term}`iterable` object that in each iteration returns one value from each list (`for th, sig in zip(gnb.theta_,gnb.sigma_)`)
- we build a [list comprehension](#listcomprehension) (`[]`) 
- so that `th` is from `gnb.theta_` and `sig` is from `gnb.sigma_` 
- we use `np.random.multivariate_normal` to get `N=`{eval}`N` samples. We can change this to anything, but I chose {eval}`N` beacuse the original data also had 50 samples per class.
- to create the matrix from the vector
of variances we multiply by `np.eye(4)`  which is the identity matrix or a matrix
with 1 on the diagonal and 0 elsewhere. because in a general [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) the second parameter is actually a covariance
matrix.  This describes both the variance of each individual feature and the
correlation of the features.  Since Gaussian Naive Bayes is Naive it assumes the features are independent or have 0 correlation.  So,  
-  stack the groups for each species together with `np.concatenate` (like `pd.concat` but works on numpy objects   and `np.random.multivariate_normal` returns numpy arrays not data frames)
-  put all of that in a DataFrame using the feature names as the columns.
-  add a species column, by repeating each species 20 times
`[c]*N for c in gnb.classes_` and then unpack that into a single list instead of
as list of lists.  

::::::{exercise}
Try breaking the cell above down and using inspection to see what each individual part does
::::::::


Now that we have simulated data (also known as synthetic) that represents our what our model knows, we can plot it the same way we plotted [the actual iris data](#irispairplot).
```{code-cell} ipython3
:label: simirispairplot
sns.pairplot(data =gnb_df, hue='species')
```

These look pretty similar. 

To help compare visually here they are in tabs so you can toggle back and forth. 
:::::::{tab-set}
::::{tab} Original
:::{embed} irispairplot
:::
::::
::::{tab} Simulated
:::{embed} simirispairplot
:::
::::
:::::::

## Think Ahead
:::::{attention}
We will work with this data in the next class
::::::
Does this data meet the assumptions of Gaussian Naive Bayes?

```{code-cell} ipython3
corner_data = 'https://raw.githubusercontent.com/rhodyprog4ds/06-naive-bayes/f425ba121cc0c4dd8bcaa7ebb2ff0b40b0b03bff/data/dataset6.csv'
df6= pd.read_csv(corner_data,usecols=[1,2,3])
sns.pairplot(data=df6, hue='char',hue_order=['A','B'])
```


## Questions

### Why some of our numbers were different than yours? 

The `train_test_split` function is random, so each time it is run it will give different results, unless the `random_state` is set so that it uses a particular sequence of psuedo-random numbers. 


### is `species` as in what type of flower it is?
yes

### what is X_train and y_train doing?

The original data had 5 columns

```{code-cell} ipython3
iris_df.head()
```

Since we will do classification which is supervised learning, the {term}`training data` has two parts: the features and targets

X is only the {term}`feature` columns, here {eval}`feature_vars`

```{code-cell} ipython3
X_train.head()
```

y is only the {term}`target` column, here {eval}`target_var`:

```{code-cell} ipython3
y_train.head()
```

We can see that these have the same rows selected. 


We can also confirm that these are different rows from the test data

```{code-cell} ipython3
np.sum([xti in X_test.index for xti in X_train.index])
```

Since the above came out to 0, we know tht for every element of `X_train.index` it does not appear in `X_test.index`

### I would like to know more about making model predictions

We will be doing more of this over the next few classes! 


###  What are the different names for parameters within GNB?

The Gausian Naive Bayes classifier has two main model parameters the mean and variance as a [Gaussian Distribution](#dist:gaussian) does. There are a few others, you can read about in the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), but are not required. 

### What is  `np.eye()` what does that mean and why use 4? 
`np.eye(D)` creates an Identity matrix of dimension D. We used {eval}`len(feature_vars)` because there are {eval}`len(feature_vars)` features ({eval}`feature_vars`) in the data. 

```{code-cell} ipython3
np.eye(4)
```
### do we only open 1 pull request for assignment 3?
Yes

### How much can classes overlap before it becomes an issue? 



The classes have to be separable to get a perfect classification, if they're not perfectly separable, then the classifier will make errors. The answer to this question depends on the cost of an error.  

For the current split (with random_state {eval}`random_state_choice`) we got a good, but not perfect [score](#gnbscore)

The test set here must overlap a little: 
```{code-cell} ipython3
test_df = X_test
test_df['species'] = y_test
sns.pairplot(test_df,hue='species')

```

If you look in the `petal_width` vs `petal_length` sub plots there are some points that almost completly overlap. 

Remember, when we had `random_state=0`, we got a perfect score. Let's do that again: 

```{code-cell} ipython3
X_train0, X_test0, y_train0, y_test0 = train_test_split(iris_df[feature_vars],
                                                    iris_df[target_var],
                                                    random_state=0)
gnb.fit(X_train0,y_train0).score(X_test0,y_test0)
```

This then has no overlap. 

```{code-cell} ipython3
test_df = X_test0
test_df['species'] = y_test0
sns.pairplot(test_df,hue='species')

```

