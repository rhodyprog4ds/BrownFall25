---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.18.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---



# Intro to NLP- representing text data


```{code-cell} ipython3
from sklearn.feature_extraction import text
from sklearn.metrics.pairwise import euclidean_distances
from sklearn import datasets
import pandas as pd
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import euclidean_distances
import pandas as pd

ng_X,ng_y = datasets.fetch_20newsgroups(categories =['comp.graphics','sci.crypt'],
                                       return_X_y = True)
```


All of the machine leanring models we have seen only use numerical features organized into a table with one row per samplea and one column per feature.

That's actually generally true.  ALl ML models require numerical features, at some point. The process of taking data that is not numerical and tabular, which is called unstrucutred, into strucutred (tabular) format we require is called feature extraction.  There are many, many ways to do that.  We'll see a few over the course of the rest of the semester.  Some more advanced models hide the feature extraction, by putting it in the same function, but it's always there.



## Terms


- document: unit of text we’re analyzing (one sample)
- token: sequence of characters in some particular document that are grouped together as a useful semantic unit for processing (basically a word)
- stop words: no meaning, we don’t need them (like a, the, an,). Note that this is context dependent
- dictionary: all of the possible words that a given system knows how to process



## Bag of Words Representionat

We're going to learn a represetnation called the bag of words.  It ignores the order of the words within a document. To do this, we'll first extract all of the tokens (tokenize) the docuemtns and then count how mnay times each word appears.  This will be our numerical representation of the data.  


::::{seealso} 
[Transformers](https://scikit-learn.org/stable/data_transforms.html) are another broad class of sklearn objects.  We've seen Estimators mostly so far. 

These are not the same as the `transformer` model that LLMs are based on. 

We're focusing on the [text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for now.
:::::::::

In order to analyze text we need some text, let's use a small sentence: 

```{code-cell} ipython3
sentence = 'Awesome, extra credit!'
```
Then we initialize our transformer, and use the fit transform method to fit the vectorizer model and apply it to this sentence({eval}`sentence`).

```{code-cell} ipython3
counts = CountVectorizer()
```

```{code-cell} ipython3
:label: singletransform
counts.fit_transform([sentence])
```


We see it returns a sparse matrix.  A sparse matrix means that it has a lot of 0s in it and so we only represent the data.  

:::::{attention} What is a Sparse Matrix?
For example
```{code-cell} ipython3
mfull = np.asarray([[1,0,0,0,0],[0,0,1,0,0],[0,0,0,1,0]])
mfull
```

but as a sparse matrix, we could store fewer values.
```{code-cell} ipython3
[[0,0,1],[1,2,1],[2,3,1]]# the above
```

So any matrix where the number of total values is low enough, we can store it more efficiently by tracking the locations and values instead of all of the zeros.

::::::::::

To actually see it though we have to cast out of that into a regular array.



```{code-cell} ipython3
counts.fit_transform([sentence]).toarray()
```


For only one sentence it's all ones, because it only has a small vocabulary.

We can make it more interesting, by picking a second sentence
```{code-cell} ipython3
counts.vocabulary_
```

To make it more interesting, we will add another sentence: 

```{code-cell} ipython3
sentence_list = [sentence, 'the cow jumped over the moon']
```

This time we can see more information

```{code-cell} ipython3
:label: twotransform
mat = counts.fit_transform(sentence_list).toarray()
```


We see that there a row for each sentence (document) and a column for each word(token).  


```{code-cell} ipython3
mat
```


We can also examine attributes of the object.
````{margin}
```{tip}
Notice that we keep using the same tools over and over to explore how things work.  You can do this on your own, when you're learning new things. Example
code is readily available online but not all of it is well documented or
clearly explained.

Also, in a job, much, much, more of your time will be spent reading code than writing code from scratch. These strategies will help you get familiar with a new code base and get up to speed faster.
```
````

::::::::::{exercise} 
:label: counts
Fill in the following to make the sums correct:

```Python

total_occurences_per_word = mat.sum(axis=??)
total_words_per_document = mat.sum(axis=??)
```

:::::::::::::::::

::::::::::{solution} counts
:class: dropdown


To get the number of tim each word occurs we sum down the columns (`axis=0`)and to get the total words (excluding stop words) for each document we sum along the rows (`axis=1`)
```{code-cell} ipython3

total_occurences_per_word = mat.sum(axis=0)
total_words_per_document = mat.sum(axis=1)
```

```{code-cell} ipython3
total_occurences_per_word, total_words_per_document
```

:::::::::::::::::

## Classifying Text


Labels are the topic of the article 0 = computer graphics, 1 = cryptography.

```{code-cell} ipython3
type(ng_X)
```

```{code-cell} ipython3
len(ng_X)
```

the X is he actual text
```{code-cell} ipython3
:tags: ["scroll-output"]
print(ng_X[0])
```

```{code-cell} ipython3
ng_y[:3]
```

### Count Vectorization
We're going to instantiate the object and fit it two the whole dataset.


```{code-cell} ipython3
count_vec =CountVectorizer()
ng_vec = count_vec.fit_transform(ng_X)
```

Next we can look at the data little: 

```{code-cell} ipython3
ng_X[:1]
```

```{code-cell} ipython3
ng_y[:5]
```

Note that this is a *very* sparse matrix:

```{code-cell} ipython3
ng_vec
```

Since the matrix is in total, {eval}`ng_vec.shape[0]` rows (the number of documents) and {eval}`ng_vec.shape[1]`  columns (the number of words), if the matrix was stored normally, we would need to store {eval}`ng_vec.shape[0]*ng_vec.shape[1]` values.  The sparse matrix, however, only stores {eval}`ng_vec.nnz` values.  This is  {eval}`ng_vec.nnz/(ng_vec.shape[0]*ng_vec.shape[1])*100`% if the values, as in less than 1% of the values! 

The sparse matrix gives us a bit more work to do programmatically, but save SO MUCH in resources that it is worth it. 

### Train /test split

Next, we train/test split:

```{code-cell} ipython3
ng_vec_train, ng_vec_test, ng_y_train, ng_y_test = train_test_split(ng_vec,ng_y)
```

::::::::::{exercise}
:label: transformfirst

Why is it important that we transform the data first and then split the train and test data?  

What could go wrong if we for example, used `fit_transform` on the training data and then tried to use `transform` on the test data?   


:::::{hint}
Compare the tansformations we did earlier for a  [single sentence](#singletransform) and for [two sentences](#twotransform).

How are they different? 
:::::::
::::::::::::

:::::::{solution} transformfirst
:class: dropdown

It is important to transform first, because if not and there are any words in the test documents that are not in the training data it won't work.  We want the feature space for both training and test to be the same or our fit classifier cannot actually evalute the test set. 

::::::::::

### Fit and eval

Now get a classifier ready:
```{code-cell} ipython3
clf = MultinomialNB()
```

Then, as normal
```{code-cell} ipython3
clf.fit(ng_vec_train, ng_y_train)
```

and score (and we can also do all of the other scores we have seen for classification)
```{code-cell} ipython3
clf.score(ng_vec_test, ng_y_test)
```

We can predict on new articles and by transforming and then passing it to our classifierr.
```{code-cell} ipython3
article_vec = count_vec.transform(['this is about cryptography'])
clf.predict(article_vec)
```
We can see that it was confident in that prediction:
```{code-cell} ipython3
clf.predict_proba(article_vec)
```

It would be high the other way with a different sentence:

```{code-cell} ipython3
article_vec = count_vec.transform(['this is about image proecessing'])
clf.predict(article_vec)
```



If we make something about both topics, we  can get less certain predictions:

```{code-cell} ipython3
article_vec = count_vec.transform(['this is about encrypting images'])
clf.predict_proba(article_vec)
```

## TF-IDF

This stands for term-frequency inverse document frequency. for a document number $d$ and word number $t$ with $D$ total documents:

$$\operatorname{tf-idf(t,d)}=\operatorname{tf(t,d)} \times \operatorname{idf(t)}$$

where:
$$\operatorname{idf}(t) = \log{\frac{1 + n}{1+\operatorname{df}(t)}} + 1$$

and
- $\operatorname{df}(t)$ is the number of documents word $t$ occurs in
- $\operatorname{tf(t,d)}$ is te number of times word $t$ occurs in document $d$



then `sklearn` also normalizes as follows:

$$v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}$$

```{code-cell} ipython3
tfidf = text.TfidfTransformer()
ng_tfidf = tfidf.fit_transform(ng_vec)
```


## Other embeddings

There are other types of embeddings that are generaly represented usign neural networks.  we talked breifly about them.

You can read more in [this tutorial on huggingface](https://huggingface.co/spaces/hesamation/primer-llm-embedding)

We also discussed some biases that embeddings end up encoding [as in this paper](https://arxiv.org/abs/1607.06520)


## Questions

### On transforming before splitting, does it break both ways (like a word in the training but not in the test) or no?

For the actual `transform` method you have, if whichever one you do second (using `transform` after `fit_transform`) has an extra word then you will have the problem. 


::::{attention}
Next week tuesday, in class will be a problem solving session with extra help for the last assignment and planning extensions
::::::