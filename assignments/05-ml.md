# Assignment 5: Modeling

[accept the assigment](https://classroom.github.com/a/q-cpZN-M)

- __Models Due: 2024-11-06__ (all models must be posted to huggingface)
- __Evaluations Due: 2024-11-10__ (notebooks due for grading)
 
You can work in a team of 3 where each of you trains 1 model and evaluates 2 others or you can work alone and train 2 models and evaluate 1 trained by another student. 

## Dataset and EDA



::::::{tab-set}

::::{tab} Group of 3
:sync: group

Work in a team of 3. Each of you needs to train a different type of model: classification, regression, or clustering. So you need to choose data appropriate to that task.  The two doing classification and clustering *may* use the same data, but do not have to. 
:::::
::::{tab} Solo
:sync: solo

You will likely need to choose at least two different datasets. one for regression and one for classifcation+ clustering. 
:::::
:::::::
Choose a dataset that is well suited for a machine learning task.

:::::{hint}
Use the [UCI ML repository](https://archive.ics.uci.edu/datasets), it  will let you filter data by the attributes of it you need. 
:::::

1. Work in a notebook named `training_<task>.ipynb` where `<task>` is either classification, regression, or clustering, depening on which you will use this dataset for. 
1. Include a basic description of the data(what the features are)
1. Describe the modeling task in your own words
1. Use EDA to determine if you expect the mdoel to work well or not. What types of mistakes do you think will happen most (think about the confusion matrix)? 
1. Split the data 80-20 and upload the 20% as a test set to the course [huggingface org](https://huggingface.co/CSC310-fall25) as a dataset or set it aside to be uploaded with your model. 

::::{note}
 You will get to reuse the above, and this dataset, for one or both of A6 and A7. 
:::::

## Basic Modeling


::::::{tab-set}
::::{tab} Group of 3 
:sync: group

Work in a team of 3. Each of you needs to train a different type of model: classification, regression, or clustering. 
:::::
::::{tab} Solo
:sync: solo

You should complete two types of training. 
:::::
:::::::

For all of the modeling, work in your `training_<task>.ipynb` notebook.  You will train and evaluate one model in this notebook. 

### Classification

1. Hypothesize which classifier from the notes will do better and why you think that. Does the data meet the assumptions of Naive Bayes? What is important about this classifier for this application? 
1. Fit your chosen classifier with the default parameters on 75% of the training data (60% of the whole dataset)
1. Inspect the model to answer the questions appropriate to your model.

    - Does this model make sense?
    - (if DT) Are there any leaves that are very small?
    - (if DT) Is this an interpretable number of levels?
    - (if GNB) do the parameters fit the data well? or do the paramters generate similar synthetic data (you can answer statistically only or with synthetic data & a plot)
1. Evaluate on the test data. Generate and interpret a classification report.
2. Interpret the model and its performance in terms of the application context in order to give a recommendation, "would you deploy this model" . Example questions to consider in your response include

  - do you think this model is good enough to use for real?
  - is this a model you would trust?
  - do you think that a more complex model should be used?
  - do you think that maybe this task cannot be done with machine learning?

:::{note}
You need to give a thorough answer to the deployment question and these bulleted questions will help you create a thorough response. 
:::

:::{note}
You need to give a thorough answer to the deployment question and these bulleted questions will help you create a thorough response. 
:::


### Clustering


1. Describe what question you would be asking in applying clustering to this dataset. What does it mean if clustering does not work well? 
2. How does this task compare to what the classification task on this dataset?
3. Apply Kmeans using the known, correct number of clusters, $K$.
4.  Evaluate how well clustering worked on the data:
    - using a true clustering metric and
    - using visualization and
    - using a clustering metric that uses the ground truth labels
5. Include a discussion of your results that addresses the following:
    - describes what the clustering means
    - what the metrics show
    - Does this clustering work better or worse than expected based on the classification performance (if you didn't complete assignment 7, also apply a classifier)

### Regression


TLDR: Fit a linear regression model, measure the fit with two metrics, and make a plot that helps visualize the result.

1. Include a basic description of the data(what the features are, units, size of dataset, etc)
2. Write  your own description of what the prediction task is, why regression is appropriate.
3. Fit a linear model on all numerical features with 75% training data.
4. Test it on 25% held out test data and measure the fit with two metrics and one plot
5. Inspect the model to answer:

    - What to the coefficients tell you?
    - What to the residuals tell you?
7. Interpret the model and its performance in terms of the application. Some questions you might want to answer in order to do this include:

  - do you think this model is good enough to use for real?
  - is this a model you would trust?
  - do you think that a more complex model should be used?
  - do you think that maybe this task cannot be done with machine learning?
1. Try fitting the model only on one feature. Justify your choice of feature based on the results above.  Plot this result.

## Share Your Model

::::::{tab-set}
::::{tab} Group of 3
:sync: group

Each of you should share the model you trained, so as a team you share 3 models. 
:::::
::::{tab} Solo
:sync: solo

You can share any single model.
:::::
:::::::

:::::{warning}
we will go over how to upload models in class on October 30
:::::::

1. Work in your `training_<task>.ipynb` notebook
1. Create a [model card](https://skops.readthedocs.io/en/stable/model_card.html) for your model, including performance, plots and limitations. 
1. Upload your model and model card to the course [hugging face organization](https://huggingface.co/CSC310-fall25) by following the [model sharing tutorial](../resources/sharing.md)

::::{tip}
A very informative model card is a way to earn `innovative`.  

For `complete` it needs to provide just enough information for your classmates to answer the [audit](#a5:audit) questions and use your model. 

You could do a first draft, get the feedback on the model card and your evaluation (what is in the `training_` notebook) and then modify it for innovoative after. 

A good model card includes detailed performance evaluation. 
::::::


## Evaluate your teams' models
::::::{tab-set}
::::{tab} Group of 3
:sync: group
Evaluate both of your team mates' models (the two tasks you did not train for)
:::::
::::{tab} Solo
:sync: solo
Evaluate a model of the type that you did not choose to train. 
:::::
:::::::

1. Create an `audit_<task_name>.ipynb` 
1. Include an introduction that answers the [Model Card Peer Review](#a5:mceval) questions below[^hover]
1. Download the model using `hf_hub_download` [see example](#getmodel)
1. Download the appropriate test data using `hf_hub_download` [see but change to a data file](#getmodel)
1. load the model using (`skio`)  [see example](#getmodel)
1. Test and evaluate the performance. Use multiple metrics, interpret all of them.  
1. Answer the [audit questions](#a5:audit) below in your `audit` notebook for grading[^hover]
1. Share your [feedback on the model card](a5:mceval) as a discussion on the `Community` tab of your classmate's model repository your classmate can make their model card better for `innovative` if they want. 

[^hover]: you can view without scrolling by hovering on the name of the questions. 

(a5:mceval)=
```
## Model Card Peer Review
1. How well did the model card prepare you to use the model?
1. What additional information might have been helpful if you were deciding between two models that can do the same thing? 
```

(a5:audit)=
```
## Model Audit
1. How does the model performance you saw compare to the model card?
1. Do you think this model works well? what are the weaknesses or strengths? 
```

::::{important}
It is **okay** if the models do not perform very well, but it **is** important that if a model you audit does not perform well that you see that. 

No one's grade will be based on how well the model works, only that choices are reasonable and interpretted well. 
:::::


## Submission

You will all submit to your own portfolio repo whether you work alone or in a group. 

1. Export [notebooks](#notebooklist) as myst markdown (by [installing jupytext](https://jupytext.readthedocs.io/en/latest/install.html) which should include [frontend features](https://jupytext.readthedocs.io/en/latest/jupyterlab-extension.html) )
1. Upload (or push) to a branch called `assignment5` **in your individual portfolio**
1. Open a [PR](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request)


(notebooklist)=
::::::{tab-set}
::::{tab} Group of 3
:sync: group

**each person submits**
1. one `training_<task>.md` where `<task>` is one of {`classification`, `clustering` or `regression`}
1. two `audit_<task>.md` where `<task>` is one of {`classification`, `clustering` or `regression`}

e.g. you might submit `training_classification.md`, `audit_clustering.md` and `audit_regression.md`

:::::
::::{tab} Solo
:sync: solo

1. two `training_<task>.md` where `<task>` is one of {`classification`, `clustering` or `regression`}
1. one `audit_<task>.md` where `<task>` is one of {`classification`, `clustering` or `regression`}

e.g. you might submit `training_classification.md`, `training_clustering.md` and `audit_regression.md`
:::::
:::::::
