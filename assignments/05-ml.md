# Assignment 5: Modeling

[accept the assigment](https://classroom.github.com/a/q-cpZN-M)

- __Models Due: 2024-11-06__
- __Audits Due: 2024-11-10__
 
You can work in a team of 3 where each of you trains 1 model and audits 2 or you can work alone and train 3 models. 

## Dataset and EDA


Choose a dataset that is well suited for a machine learning task.

:::::{hint}
Use the [UCI ML repository](https://archive.ics.uci.edu/datasets), it  will let you filter data by the attributes of it you need. 
:::::

1. Work in a notebook named `training_<task>.ipynb` where `<task>` is either classification, regression, or clustering. 
1. Include a basic description of the data(what the features are)
1. Describe the classification task in your own words
1. Use EDA to determine if you expect the classification to get a high accuracy or not. What types of mistakes do you think will happen most (think about the confusion matrix)? 
1. Split the data 80-20 and upload the 20% as a test set to the course [huggingface org](https://huggingface.co/CSC310-fall25) as a dataset

::::{note}
 You will get to reuse the above, and this dataset, for one or both of A6 and A7. 
:::::

## Basic Modeling

::::::{attention}
If you work in a team of 3, you only need to do one of the following sections. 
:::::::::


### Classification

1. Hypothesize which classifier from the notes will do better and why you think that. Does the data meet the assumptions of Naive Bayes? What is important about this classifier for this application? 
1. Fit your chosen classifier with the default parameters on 75% of the training data (60% of the whole dataset)
1. Inspect the model to answer the questions appropriate to your model.

    - Does this model make sense?
    - (if DT) Are there any leaves that are very small?
    - (if DT) Is this an interpretable number of levels?
    - (if GNB) do the parameters fit the data well? or do the paramters generate similar synthetic data (you can answer statistically only or with synthetic data & a plot)
1. Evaluate on the test data. Generate and interpret a classification report.
2. Interpret the model and its performance in terms of the application context in order to give a recommendation, "would you deploy this model" . Example questions to consider in your response include

  - do you think this model is good enough to use for real?
  - is this a model you would trust?
  - do you think that a more complex model should be used?
  - do you think that maybe this task cannot be done with machine learning?

:::{note}
You need to give a thorough answer to the deployment question and these bulleted questions will help you create a thorough response. 
:::

:::{note}
You need to give a thorough answer to the deployment question and these bulleted questions will help you create a thorough response. 
:::


### Clustering


1. Describe what question you would be asking in applying clustering to this dataset. What does it mean if clustering does not work well? 
2. How does this task compare to what the classification task on this dataset?
3. Apply Kmeans using the known, correct number of clusters, $K$.
4.  Evaluate how well clustering worked on the data:
    - using a true clustering metric and
    - using visualization and
    - using a clustering metric that uses the ground truth labels
5. Include a discussion of your results that addresses the following:
    - describes what the clustering means
    - what the metrics show
    - Does this clustering work better or worse than expected based on the classification performance (if you didn't complete assignment 7, also apply a classifier)

### Regression


TLDR: Fit a linear regression model, measure the fit with two metrics, and make a plot that helps visualize the result.

1. Include a basic description of the data(what the features are, units, size of dataset, etc)
2. Write  your own description of what the prediction task is, why regression is appropriate.
3. Fit a linear model on all numerical features with 75% training data.
4. Test it on 25% held out test data and measure the fit with two metrics and one plot
5. Inspect the model to answer:

    - What to the coefficients tell you?
    - What to the residuals tell you?
7. Interpret the model and its performance in terms of the application. Some questions you might want to answer in order to do this include:

  - do you think this model is good enough to use for real?
  - is this a model you would trust?
  - do you think that a more complex model should be used?
  - do you think that maybe this task cannot be done with machine learning?
1. Try fitting the model only on one feature. Justify your choice of feature based on the results above.  Plot this result.

## Share Your Model

::::{attention}
If you work alone, you can choose to share your model for only one of the 3 tasks
::::::

:::::{warning}
we will go over how to upload models in class on October 30
:::::::

1. Work in your `training_<task>.ipynb` notebook
1. Create a [model card](https://skops.readthedocs.io/en/stable/model_card.html) for your model, including performance, plots and limitations. 
1. Upload your model and model card to the course [hugging face organization](https://huggingface.co/CSC310-fall25)


## Evaluate your teams' models
::::{attention}
If you work alone, you can evaluate any single model that a classmate uploads.  
::::::

1. Create an `eval_<task_name>.ipynb` 
(mceval)=
1. Include an introduction that answers the [Model Card Peer Review](#mceval) questions
1. Download the model using `hf_hub_download`
1. Download the apporpriate test data using `hf_hub_download`
1. load the model using (`skio`) 
1. Test and evaluate the performance. 
1. Answer the audit questions


(mceval)=
```
## Model Card Peer Review
1. Did the model card provide you with everything you want to know to use the model
```

(audit)=
```
## Model Audit
1. How does the model performance you saw compare to the model card
1. Do you think this model works well? what are the weaknesses
```

::::{important}
It is **okay** if the models do not perform very well, but it **is** important that if a model you audit does not perform well that you see that. 

No one's grade will be based on how well the model works, only that choices are reasonable and interpretted well. 
:::::
